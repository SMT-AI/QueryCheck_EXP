{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "143fc13b-832a-4aa6-b1aa-4bb76224039a",
   "metadata": {},
   "source": [
    "## FasText (Scractch) Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf121e75-701b-413d-9474-76a8d0c2f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd7398b7-bff1-4e8b-97a9-21b6b124ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubwordTokenizer:\n",
    "    def __init__(self, min_ngram: int = 3, max_ngram: int = 6):\n",
    "        self.min_ngram = min_ngram\n",
    "        self.max_ngram = max_ngram\n",
    "        self.vocab = set()\n",
    "        self.subword2idx = {'<PAD>': 0}  # Add padding token\n",
    "        self.idx2subword = {0: '<PAD>'}\n",
    "        self.word2subwords = defaultdict(list)\n",
    "        \n",
    "    def generate_subwords(self, word: str) -> List[str]:\n",
    "        word = f\"<{word}>\"  # Add boundaries\n",
    "        subwords = []\n",
    "        \n",
    "        # Add the word itself\n",
    "        subwords.append(word)\n",
    "        \n",
    "        # Generate n-grams\n",
    "        for n in range(self.min_ngram, self.max_ngram + 1):\n",
    "            for i in range(len(word) - n + 1):\n",
    "                subwords.append(word[i:i+n])\n",
    "                \n",
    "        return subwords\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 5):\n",
    "        # Count subword frequencies\n",
    "        subword_counts = Counter()\n",
    "        \n",
    "        for text in texts:\n",
    "            words = self._preprocess_text(text)\n",
    "            for word in words:\n",
    "                subwords = self.generate_subwords(word)\n",
    "                subword_counts.update(subwords)\n",
    "                self.word2subwords[word] = subwords\n",
    "        \n",
    "        # Filter by frequency and create vocabulary\n",
    "        filtered_subwords = [sw for sw, count in subword_counts.items() \n",
    "                           if count >= min_freq]\n",
    "        \n",
    "        # Create mappings (starting from 1 since 0 is for padding)\n",
    "        for idx, subword in enumerate(filtered_subwords, start=1):\n",
    "            self.subword2idx[subword] = idx\n",
    "            self.idx2subword[idx] = subword\n",
    "            self.vocab.add(subword)\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> List[str]:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text.split()\n",
    "    \n",
    "    def encode_word(self, word: str) -> torch.Tensor:\n",
    "        subwords = self.generate_subwords(word)\n",
    "        indices = [self.subword2idx[sw] for sw in subwords \n",
    "                  if sw in self.subword2idx]\n",
    "        if not indices:  # Handle unknown words\n",
    "            return torch.tensor([0])  # Use padding index\n",
    "        return torch.tensor(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d409fa13-9f0d-4f50-9603-6fe639f10287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable-length sequences\"\"\"\n",
    "    # Separate target and context\n",
    "    target_sequences = [item[0] for item in batch]\n",
    "    context_sequences = [item[1] for item in batch]\n",
    "    \n",
    "    # Get lengths for packing\n",
    "    target_lengths = torch.tensor([len(seq) for seq in target_sequences])\n",
    "    context_lengths = torch.tensor([len(seq) for seq in context_sequences])\n",
    "    \n",
    "    # Pad sequences\n",
    "    target_padded = pad_sequence(target_sequences, batch_first=True, padding_value=0)\n",
    "    context_padded = pad_sequence(context_sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return (target_padded, target_lengths), (context_padded, context_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7d7756b-4bd7-481a-96e8-53d941a32d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], tokenizer: SubwordTokenizer, \n",
    "                 window_size: int = 5):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.window_size = window_size\n",
    "        self.data = []\n",
    "        \n",
    "        # Create training pairs\n",
    "        for text in texts:\n",
    "            words = tokenizer._preprocess_text(text)\n",
    "            for i, target_word in enumerate(words):\n",
    "                # Get context words within window\n",
    "                context_indices = list(range(max(0, i - window_size), i)) + \\\n",
    "                                list(range(i + 1, min(len(words), i + window_size + 1)))\n",
    "                \n",
    "                target_subwords = tokenizer.encode_word(target_word)\n",
    "                \n",
    "                for j in context_indices:\n",
    "                    context_subwords = tokenizer.encode_word(words[j])\n",
    "                    self.data.append((target_subwords, context_subwords))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf4f3ba8-10af-4662-a85e-876b78f8d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super(FastTextModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: [batch_size, max_seq_len]\n",
    "        # lengths shape: [batch_size]\n",
    "        \n",
    "        # Get embeddings\n",
    "        embedded = self.embeddings(x)  # [batch_size, max_seq_len, embedding_dim]\n",
    "        \n",
    "        # Create mask for padding\n",
    "        mask = (x != 0).float().unsqueeze(-1)  # [batch_size, max_seq_len, 1]\n",
    "        \n",
    "        # Apply mask and average\n",
    "        masked_embedded = embedded * mask\n",
    "        summed = masked_embedded.sum(dim=1)  # [batch_size, embedding_dim]\n",
    "        averaged = summed / lengths.float().unsqueeze(1)  # [batch_size, embedding_dim]\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        output = self.output(averaged)  # [batch_size, vocab_size]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70d1b4b9-23ad-4257-8ffd-c48ff6cabdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fasttext(model: FastTextModel, \n",
    "                  train_loader: DataLoader,\n",
    "                  num_epochs: int,\n",
    "                  learning_rate: float = 0.001,\n",
    "                  device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, ((target_words, target_lengths), (context_words, context_lengths)) in enumerate(train_loader):\n",
    "            target_words = target_words.to(device)\n",
    "            context_words = context_words.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "            context_lengths = context_lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(target_words, target_lengths)\n",
    "            \n",
    "            # Calculate target for loss\n",
    "            context_target = torch.argmax(model(context_words, context_lengths), dim=1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, context_target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2eb495ea-8b7a-464c-820c-1911e2305a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_fasttext(model: FastTextModel,\n",
    "                      fine_tune_texts: List[str],\n",
    "                      tokenizer: SubwordTokenizer,\n",
    "                      num_epochs: int = 5,\n",
    "                      learning_rate: float = 0.0001):\n",
    "    \"\"\"Fine-tune the model on domain-specific data\"\"\"\n",
    "    \n",
    "    # Create fine-tuning dataset\n",
    "    fine_tune_dataset = FastTextDataset(fine_tune_texts, tokenizer)\n",
    "    fine_tune_loader = DataLoader(\n",
    "        fine_tune_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Use smaller learning rate for fine-tuning\n",
    "    train_fasttext(model, fine_tune_loader, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d74538e-0932-4b55-be2d-5b7925cd0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(model: FastTextModel,\n",
    "                   word: str,\n",
    "                   tokenizer: SubwordTokenizer,\n",
    "                   device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Get the vector representation of a word\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        subwords = tokenizer.encode_word(word).unsqueeze(0).to(device)\n",
    "        lengths = torch.tensor([len(subwords[0])]).to(device)\n",
    "        embedded = model.embeddings(subwords)\n",
    "        mask = (subwords != 0).float().unsqueeze(-1)\n",
    "        masked_embedded = embedded * mask\n",
    "        return torch.sum(masked_embedded, dim=1) / lengths.float().unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b77dafa9-9248-4cb6-89f2-3300894e4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_products(model: FastTextModel,\n",
    "                        query: str,\n",
    "                        product_texts: List[str],\n",
    "                        tokenizer: SubwordTokenizer,\n",
    "                        top_k: int = 5,\n",
    "                        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Find similar products based on text similarity\"\"\"\n",
    "    \n",
    "    # Get query vector\n",
    "    query_vector = get_word_vector(model, query, tokenizer, device)\n",
    "    \n",
    "    # Calculate similarities with all products\n",
    "    similarities = []\n",
    "    for product in product_texts:\n",
    "        product_vector = get_word_vector(model, product, tokenizer, device)\n",
    "        similarity = F.cosine_similarity(query_vector, product_vector)\n",
    "        similarities.append((product, similarity.item()))\n",
    "    \n",
    "    # Sort by similarity and return top k\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "823dda6f-1061-4592-bb20-4949485f1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    \"Computer Networking: A Top-Down Approach\",\n",
    "    \"Data Communications and Networking\",\n",
    "    \"Networking All-in-One For Dummies\",\n",
    "    \"Computer Networks\",\n",
    "    \"The Linux Command Line\",\n",
    "    \"Network Warrior\",\n",
    "    \"Internetworking with TCP/IP\",\n",
    "    \"TCP/IP Illustrated, Volume 1\",\n",
    "    \"CompTIA Network+ Guide to Networks\",\n",
    "    \"Mastering Bitcoin\",\n",
    "    \"Hacking: The Art of Exploitation\",\n",
    "    \"The Phoenix Project\",\n",
    "    \"Artificial Intelligence: A Modern Approach\",\n",
    "    \"Machine Learning Yearning\",\n",
    "    \"Deep Learning\",\n",
    "    \"Introduction to the Theory of Computation\",\n",
    "    \"The Pragmatic Programmer\",\n",
    "    \"Clean Code: A Handbook of Agile Software Craftsmanship\",\n",
    "    \"Code Complete\",\n",
    "    \"Structure and Interpretation of Computer Programs\",\n",
    "    \"Designing Data-Intensive Applications\",\n",
    "    \"Python Crash Course\",\n",
    "    \"Learning Python\",\n",
    "    \"Automate the Boring Stuff with Python\",\n",
    "    \"Introduction to Algorithms\",\n",
    "    \"Algorithms Unlocked\",\n",
    "    \"Network Security Essentials\",\n",
    "    \"Computer Security: Principles and Practice\",\n",
    "    \"Applied Cryptography\",\n",
    "    \"The C Programming Language\",\n",
    "    \"Effective Java\",\n",
    "    \"Java: The Complete Reference\",\n",
    "    \"Programming Perl\",\n",
    "    \"Head First Java\",\n",
    "    \"Python Programming: An Introduction to Computer Science\",\n",
    "    \"C++ Primer\",\n",
    "    \"Effective C++\",\n",
    "    \"Eloquent JavaScript\",\n",
    "    \"Modern Operating Systems\",\n",
    "    \"Operating System Concepts\",\n",
    "    \"Computer Organization and Design\",\n",
    "    \"Digital Design and Computer Architecture\",\n",
    "    \"Microservices Patterns\",\n",
    "    \"Docker Deep Dive\",\n",
    "    \"Kubernetes Up & Running\",\n",
    "    \"Building Microservices\",\n",
    "    \"The Go Programming Language\",\n",
    "    \"The Rust Programming Language\",\n",
    "    \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\",\n",
    "    \"Artificial Intelligence with Python\",\n",
    "    \"Neural Networks and Deep Learning\",\n",
    "    \"Blockchain Basics\",\n",
    "    \"Cloud Computing: Concepts, Technology & Architecture\",\n",
    "    \"Mastering Cloud Computing\",\n",
    "    \"Cloud Native Patterns\",\n",
    "    \"AWS Certified Solutions Architect Study Guide\",\n",
    "    \"Kubernetes for Developers\",\n",
    "    \"Introduction to Machine Learning with Python\",\n",
    "    \"Hands-On Networking Fundamentals\",\n",
    "    \"Networking Basics\",\n",
    "    \"Wireless Communications: Principles and Practice\",\n",
    "    \"Cisco CCNA 200-301 Official Cert Guide\",\n",
    "    \"Networking Fundamentals\",\n",
    "    \"Building Scalable Web Applications\",\n",
    "    \"The Art of Scalability\",\n",
    "    \"Pro Git\",\n",
    "    \"Linux Kernel Development\",\n",
    "    \"Shell Scripting: How to Automate Command Line Tasks\",\n",
    "    \"Advanced Programming in the UNIX Environment\",\n",
    "    \"Network Programmability and Automation\",\n",
    "    \"Networking for Systems Administrators\",\n",
    "    \"Computer Vision: Algorithms and Applications\",\n",
    "    \"Digital Image Processing\",\n",
    "    \"Natural Language Processing with Python\",\n",
    "    \"Practical Packet Analysis\",\n",
    "    \"Wireshark for Security Professionals\",\n",
    "    \"Security Engineering\",\n",
    "    \"Certified Ethical Hacker (CEH) Study Guide\",\n",
    "    \"Linux Bible\",\n",
    "    \"Learning Kali Linux\",\n",
    "    \"Cisco Networking Essentials\",\n",
    "    \"CompTIA Security+ Study Guide\",\n",
    "    \"Linux Networking Cookbook\",\n",
    "    \"Networking for Dummies\",\n",
    "    \"Network Simulation Experiments Manual\",\n",
    "    \"Cisco CCNP and CCIE Enterprise Core\",\n",
    "    \"Mastering Python Networking\",\n",
    "    \"Fundamentals of Database Systems\",\n",
    "    \"SQL in 10 Minutes, Sams Teach Yourself\",\n",
    "    \"Database System Concepts\",\n",
    "    \"PostgreSQL: Up and Running\",\n",
    "    \"MySQL High Availability\",\n",
    "    \"The Definitive Guide to Django\",\n",
    "    \"Fluent Python\",\n",
    "    \"High Performance MySQL\",\n",
    "    \"Network Management Fundamentals\",\n",
    "    \"DNS and BIND\",\n",
    "    \"Web Security for Developers\",\n",
    "    \"Cybersecurity Essentials\",\n",
    "    \"Cyber War: The Next Threat to National Security\",\n",
    "    \"Cyber-Physical Systems\",\n",
    "    \"Penetration Testing\",\n",
    "    \"The Cyber Effect\",\n",
    "    \"CISSP All-in-One Exam Guide\",\n",
    "    \"AWS Certified Developer Official Study Guide\",\n",
    "    \"CompTIA A+ Certification All-in-One Exam Guide\",\n",
    "    \"Operating Systems: Three Easy Pieces\",\n",
    "    \"The Algorithm Design Manual\",\n",
    "    \"Computer Graphics: Principles and Practice\",\n",
    "    \"Learning Computer Architecture with Raspberry Pi\",\n",
    "    \"Embedded Systems: Introduction to ARM Cortex-M Microcontrollers\",\n",
    "    \"Hands-On Embedded Programming with C++\",\n",
    "    \"ARM System Developer's Guide\",\n",
    "    \"Make: Electronics\",\n",
    "    \"Programming Arduino: Getting Started with Sketches\",\n",
    "    \"Raspberry Pi Cookbook\",\n",
    "    \"Computer Networking Problems and Solutions\",\n",
    "    \"Computer Science Distilled\",\n",
    "    \"C Programming Absolute Beginner's Guide\",\n",
    "    \"JavaScript: The Good Parts\",\n",
    "    \"You Don't Know JS\",\n",
    "    \"Learn C the Hard Way\",\n",
    "    \"JavaScript and JQuery: Interactive Front-End Web Development\",\n",
    "    \"Software Engineering at Google\",\n",
    "    \"The Mythical Man-Month\",\n",
    "    \"An Introduction to Statistical Learning\",\n",
    "    \"Practical Data Science with R\",\n",
    "    \"Machine Learning for Dummies\",\n",
    "    \"Reinforcement Learning: An Introduction\",\n",
    "    \"Deep Reinforcement Learning Hands-On\",\n",
    "    \"Computer Vision with OpenCV\",\n",
    "    \"Learning OpenCV 4 Computer Vision\",\n",
    "    \"Hands-On GPU Programming with CUDA\",\n",
    "    \"CUDA by Example\",\n",
    "    \"Parallel Programming with OpenMP\",\n",
    "    \"GPU Programming for Beginners\",\n",
    "    \"Computer Systems: A Programmer's Perspective\",\n",
    "    \"Systems Performance: Enterprise and the Cloud\",\n",
    "    \"Computer Architecture: A Quantitative Approach\",\n",
    "    \"Linux Administration Handbook\",\n",
    "    \"Network Analysis with Wireshark\",\n",
    "    \"Computer Networking: Principles, Protocols and Practice\",\n",
    "    \"Networking for Game Developers\",\n",
    "    \"Programming Massively Parallel Processors\",\n",
    "    \"Learning Spark\",\n",
    "    \"Spark: The Definitive Guide\",\n",
    "    \"Kafka: The Definitive Guide\",\n",
    "    \"Big Data: Principles and Best Practices\",\n",
    "    \"Data Science from Scratch\",\n",
    "    \"Artificial Intelligence by Example\",\n",
    "    \"Software Architecture in Practice\",\n",
    "    \"Mastering OpenCV with Practical Computer Vision Projects\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0534532-5472-4785-9f57-f171185f0a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SubwordTokenizer()\n",
    "tokenizer.build_vocab(sample_texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "013d6ee6-252e-456e-a7ea-7eb07ae810fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FastTextDataset(sample_texts, tokenizer)\n",
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn  # Use custom collate function\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dad8ad7-63ed-417c-a155-845734f34179",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastTextModel(\n",
    "        vocab_size=len(tokenizer.subword2idx),\n",
    "        embedding_dim=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd471110-06e5-40f8-b005-5c69511b0d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 6.2936\n",
      "Epoch [2/20], Loss: 5.8324\n",
      "Epoch [3/20], Loss: 4.8292\n",
      "Epoch [4/20], Loss: 2.8538\n",
      "Epoch [5/20], Loss: 1.2899\n",
      "Epoch [6/20], Loss: 0.7737\n",
      "Epoch [7/20], Loss: 0.6185\n",
      "Epoch [8/20], Loss: 0.5645\n",
      "Epoch [9/20], Loss: 0.5377\n",
      "Epoch [10/20], Loss: 0.5219\n",
      "Epoch [11/20], Loss: 0.5120\n",
      "Epoch [12/20], Loss: 0.5033\n",
      "Epoch [13/20], Loss: 0.4976\n",
      "Epoch [14/20], Loss: 0.4926\n",
      "Epoch [15/20], Loss: 0.4868\n",
      "Epoch [16/20], Loss: 0.4827\n",
      "Epoch [17/20], Loss: 0.4791\n",
      "Epoch [18/20], Loss: 0.4753\n",
      "Epoch [19/20], Loss: 0.4708\n",
      "Epoch [20/20], Loss: 0.4666\n"
     ]
    }
   ],
   "source": [
    "train_fasttext(model, dataloader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf188533-4e8f-416b-873a-26f6a5e636c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_specific_texts = [\n",
    "        \"Networking for Dummies\",\n",
    "    \"Network Simulation Experiments Manual\",\n",
    "    \"Cisco CCNP and CCIE Enterprise Core\",\n",
    "    \"Mastering Python Networking\",\n",
    "    \"Fundamentals of Database Systems\",\n",
    "    \"SQL in 10 Minutes, Sams Teach Yourself\",\n",
    "    \"Database System Concepts\",\n",
    "    \"PostgreSQL: Up and Running\",\n",
    "    \"MySQL High Availability\",\n",
    "    \"The Definitive Guide to Django\",\n",
    "    \"Fluent Python\",\n",
    "    \"High Performance MySQL\",\n",
    "    \"Network Management Fundamentals\",\n",
    "    \"DNS and BIND\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63ddb46d-4901-4d2a-be33-bc3abcc4c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.0993\n",
      "Epoch [2/5], Loss: 1.0991\n",
      "Epoch [3/5], Loss: 1.0988\n",
      "Epoch [4/5], Loss: 1.0985\n",
      "Epoch [5/5], Loss: 1.0983\n"
     ]
    }
   ],
   "source": [
    "fine_tune_fasttext(model, domain_specific_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b5f1d7ce-2b6f-41af-9e21-618c1f3f8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"DNS and BIDN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d6f37626-a0b8-4bc1-9845-9831cc93d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_products = find_similar_products(model, query, sample_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8664b9c5-60af-4eb0-9dc6-0595d8c76ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Products similar to query: DNS and BIDN\n",
      "DNS and BIND: 1.0000\n",
      "Clean Code: A Handbook of Agile Software Craftsmanship: 0.5898\n",
      "PostgreSQL: Up and Running: 0.5512\n",
      "Cisco CCNP and CCIE Enterprise Core: 0.5367\n",
      "Shell Scripting: How to Automate Command Line Tasks: 0.5272\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProducts similar to query:\", query)\n",
    "for product, similarity in similar_products:\n",
    "    print(f\"{product}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b2d07-c46e-485c-876c-d72a999c8d8a",
   "metadata": {},
   "source": [
    "## Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "723465f9-d791-40c7-accc-9b72c8a349ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: FastTextModel, \n",
    "                tokenizer: SubwordTokenizer,\n",
    "                save_path: str):\n",
    "    \"\"\"Save both the model and tokenizer state\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab_size': len(tokenizer.subword2idx),\n",
    "        'embedding_dim': model.embeddings.embedding_dim,\n",
    "        # Save tokenizer state\n",
    "        'tokenizer_state': {\n",
    "            'min_ngram': tokenizer.min_ngram,\n",
    "            'max_ngram': tokenizer.max_ngram,\n",
    "            'vocab': tokenizer.vocab,\n",
    "            'subword2idx': tokenizer.subword2idx,\n",
    "            'idx2subword': tokenizer.idx2subword,\n",
    "            'word2subwords': dict(tokenizer.word2subwords)  # Convert defaultdict to dict\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model and tokenizer saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c19ebc35-193b-4635-8b49-b9fcd7e5fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to fastext_ComputeBooks.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = \"fastext_ComputeBooks.pt\"\n",
    "save_model(model, tokenizer, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb211d-03b5-4250-8109-09af911c7b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
